{
 "metadata": {
  "name": "",
  "signature": "sha256:fb2c57ff7f6d25a0d3778ece88c614bd1daa8726506c5652664a38d48a07bc69"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Good morning!\n",
      "=============\n",
      "\n",
      "PRELIMINARIES\n",
      "=============\n",
      "\n",
      "I have (I think) about 90 minutes of material.  This session is supposed to go for about 110 minutes.  I thought we could break it up as follows:\n",
      "\n",
      "* 30 minutes content\n",
      "* 5 minutes\n",
      "* 30 minutes content\n",
      "* 5 minutes\n",
      "* 30 minutes content\n",
      "* 10 minutes questions/discussion\n",
      "\n",
      "About me\n",
      "--------\n",
      "\n",
      "* Douglas Starnes\n",
      "* Pythonic polyglot 'ninja'\n",
      "* Memphis, TN\n",
      "* Programming languages, mobile, 'extreme web', data\n",
      "* Speaker\n",
      "* Co-director of Memphis Python User Group (MEMpy)\n",
      "* PyTennessee staff\n",
      "\n",
      "Contact\n",
      "-------\n",
      "* douglas@douglasstarnes.com (douglas.a.starnes@gmail.com)\n",
      "* http://douglasstarnes.com\n",
      "* @poweredbyaltnet\n",
      "\n",
      "NLTK is the Natural Language ToolKit.  It is a set of software, data and documentation to perform Natural Language Processing (NLP) in Python.\n",
      "\n",
      "Natural language is spoken and/or written language used by humans for communication.  We will be focusing on English (because today's host only speaks English) but NLTK has some facilities for other languages as well.\n",
      "\n",
      "Natural language processing is the automated (usually by a computer) analysis of natural language.\n",
      "\n",
      "Uses of natural language processing:\n",
      "------------------------------------\n",
      "\n",
      "* essay evaluation\n",
      "* information retrieval\n",
      "* question answering\n",
      "* sentiment analysis\n",
      "* spam detection\n",
      "* optical character recognition\n",
      "* speech recognition\n",
      "\n",
      "Getting NLTK\n",
      "------------\n",
      "\n",
      "You must have Python version 2.6+ or 3.2+ (for NLTK 3)  After that installation is simple:\n",
      "\n",
      "    pip install nltk\n",
      "\n",
      "NLTK has soft dependencies on numpy and matplotlib.\n",
      "\n",
      "You'll want to have some data, or corpora to experiment with:\n",
      "\n",
      "    nltk.download_gui()"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "CORPORA\n",
      "=======\n",
      "\n",
      "NLTK comes with over over 40 corpora or bodies of text that we can experiment with.  The corpora have a variety of features.  Here are a few of the more popular ones:\n",
      "\n",
      "* Gutenberg\n",
      "* Brown\n",
      "* Stopwords\n",
      "* Penn TreeBank\n",
      "* WordNet\n",
      "\n",
      "Corpora are imported as modules:\n",
      "\n",
      "Let's take a look at a few of them in code.\n",
      "\n",
      "Gutenberg\n",
      "---------"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "import nltk\n",
      "from nltk.corpus import gutenberg\n",
      "gutenberg"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 150,
       "text": [
        "<PlaintextCorpusReader in '/Users/douglasstarnes/nltk_data/corpora/gutenberg'>"
       ]
      }
     ],
     "prompt_number": 150
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The Gutenberg corpus is a **`PlaintextCorpusReader`** and contains a collection of text documents with no metadata."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "for fileid in gutenberg.fileids():\n",
      "    print fileid"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "austen-emma.txt\n",
        "austen-persuasion.txt\n",
        "austen-sense.txt\n",
        "bible-kjv.txt\n",
        "blake-poems.txt\n",
        "bryant-stories.txt\n",
        "burgess-busterbrown.txt\n",
        "carroll-alice.txt\n",
        "chesterton-ball.txt\n",
        "chesterton-brown.txt\n",
        "chesterton-thursday.txt\n",
        "edgeworth-parents.txt\n",
        "melville-moby_dick.txt\n",
        "milton-paradise.txt\n",
        "shakespeare-caesar.txt\n",
        "shakespeare-hamlet.txt\n",
        "shakespeare-macbeth.txt\n",
        "whitman-leaves.txt\n"
       ]
      }
     ],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "fileid = 'shakespeare-macbeth.txt'\n",
      "words = gutenberg.words(fileid)\n",
      "words"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 152,
       "text": [
        "['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', ...]"
       ]
      }
     ],
     "prompt_number": 152
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "len(words)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 153,
       "text": [
        "23140"
       ]
      }
     ],
     "prompt_number": 153
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "sents = gutenberg.sents(fileid)\n",
      "sents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 154,
       "text": [
        "[['[', 'The', 'Tragedie', 'of', 'Macbeth', 'by', 'William', 'Shakespeare', '1603', ']'], ['Actus', 'Primus', '.'], ...]"
       ]
      }
     ],
     "prompt_number": 154
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "len(sents)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 155,
       "text": [
        "1907"
       ]
      }
     ],
     "prompt_number": 155
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "raw = gutenberg.raw(fileid)\n",
      "raw[:1000]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 184,
       "text": [
        "\"[The Tragedie of Macbeth by William Shakespeare 1603]\\n\\n\\nActus Primus. Scoena Prima.\\n\\nThunder and Lightning. Enter three Witches.\\n\\n  1. When shall we three meet againe?\\nIn Thunder, Lightning, or in Raine?\\n  2. When the Hurley-burley's done,\\nWhen the Battaile's lost, and wonne\\n\\n   3. That will be ere the set of Sunne\\n\\n   1. Where the place?\\n  2. Vpon the Heath\\n\\n   3. There to meet with Macbeth\\n\\n   1. I come, Gray-Malkin\\n\\n   All. Padock calls anon: faire is foule, and foule is faire,\\nHouer through the fogge and filthie ayre.\\n\\nExeunt.\\n\\n\\nScena Secunda.\\n\\nAlarum within. Enter King Malcome, Donalbaine, Lenox, with\\nattendants,\\nmeeting a bleeding Captaine.\\n\\n  King. What bloody man is that? he can report,\\nAs seemeth by his plight, of the Reuolt\\nThe newest state\\n\\n   Mal. This is the Serieant,\\nWho like a good and hardie Souldier fought\\n'Gainst my Captiuitie: Haile braue friend;\\nSay to the King, the knowledge of the Broyle,\\nAs thou didst leaue it\\n\\n   Cap. Doubtfull it stood,\\nAs two spent Swimmers, t\""
       ]
      }
     ],
     "prompt_number": 184
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "len(raw)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 157,
       "text": [
        "100351"
       ]
      }
     ],
     "prompt_number": 157
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Stopwords\n",
      "---------\n",
      "A 'stopword' is a word that usually is filtered out.  This includes words such as 'the', 'and', 'or', etc.  Note that there is no definitive list but NLTK has a fairly complete one."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import stopwords\n",
      "words = stopwords.words('english')\n",
      "for word in words[:25]: print word,"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i me my myself we our ours ourselves you your yours yourself yourselves he him his himself she her hers herself it its itself they\n"
       ]
      }
     ],
     "prompt_number": 158
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NLTK also has lists of stopwords in many languages."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "languages = stopwords.fileids()\n",
      "for lang in languages:\n",
      "    words = stopwords.words(lang)\n",
      "    print lang\n",
      "    for word in words[:25]: print word,\n",
      "    print '\\n'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "danish\n",
        "og i jeg det at en den til er som p\u00e5 de med han af for ikke der var mig sig men et har om \n",
        "\n",
        "dutch\n",
        "de en van ik te dat die in een hij het niet zijn is was op aan met als voor had er maar om hem \n",
        "\n",
        "english\n",
        "i me my myself we our ours ourselves you your yours yourself yourselves he him his himself she her hers herself it its itself they \n",
        "\n",
        "finnish\n",
        "olla olen olet on olemme olette ovat ole oli olisi olisit olisin olisimme olisitte olisivat olit olin olimme olitte olivat ollut olleet en et ei \n",
        "\n",
        "french\n",
        "au aux avec ce ces dans de des du elle en et eux il je la le leur lui ma mais me m\u00eame mes moi \n",
        "\n",
        "german\n",
        "aber alle allem allen aller alles als also am an ander andere anderem anderen anderer anderes anderm andern anderr anders auch auf aus bei bin \n",
        "\n",
        "hungarian\n",
        "a ahogy ahol aki akik akkor alatt \u00e1ltal \u00e1ltal\u00e1ban amely amelyek amelyekben amelyeket amelyet amelynek ami amit amolyan am\u00edg amikor \u00e1t abban ahhoz annak arra \n",
        "\n",
        "italian\n",
        "ad al allo ai agli all agl alla alle con col coi da dal dallo dai dagli dall dagl dalla dalle di del dello dei \n",
        "\n",
        "norwegian\n",
        "og i jeg det at en et den til er som p\u00e5 de med han av ikke ikkje der s\u00e5 var meg seg men ett \n",
        "\n",
        "portuguese\n",
        "de a o que e do da em um para com n\u00e3o uma os no se na por mais as dos como mas ao ele \n",
        "\n",
        "russian\n",
        "\u0438 \u0432 \u0432\u043e \u043d\u0435 \u0447\u0442\u043e \u043e\u043d \u043d\u0430 \u044f \u0441 \u0441\u043e \u043a\u0430\u043a \u0430 \u0442\u043e \u0432\u0441\u0435 \u043e\u043d\u0430 \u0442\u0430\u043a \u0435\u0433\u043e \u043d\u043e \u0434\u0430 \u0442\u044b \u043a \u0443 \u0436\u0435 \u0432\u044b \u0437\u0430 \n",
        "\n",
        "spanish\n",
        "de la que el en y a los del se las por un para con no una su al lo como m\u00e1s"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        " pero sus le \n",
        "\n",
        "swedish\n",
        "och det att i en jag hon som han p\u00e5 den med var sig f\u00f6r s\u00e5 till \u00e4r men ett om hade de av icke \n",
        "\n",
        "turkish\n",
        "acaba ama asl\u0131nda az baz\u0131 belki biri birka\u00e7 bir\u015fey biz bu \u00e7ok \u00e7\u00fcnk\u00fc da daha de defa diye e\u011fer en gibi hem hep hepsi her \n",
        "\n"
       ]
      }
     ],
     "prompt_number": 159
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Penn Treebank \n",
      "-------------\n",
      "\n",
      "The Penn Treebank organizes sentences into trees. [Example](http://en.wikipedia.org/wiki/Treebank#mediaviewer/File:The_house_at_the_end_of_the_street.jpg)\n",
      "\n",
      "Another valuable feature of the Penn Treebank is extensive part of speech tagging. [List](https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import treebank\n",
      "words = treebank.tagged_words()[:25]\n",
      "print ', '.join([\"('%s', '%s')\" % (word[0], word[1]) for word in words])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Pierre', 'NNP'), ('Vinken', 'NNP'), (',', ','), ('61', 'CD'), ('years', 'NNS'), ('old', 'JJ'), (',', ','), ('will', 'MD'), ('join', 'VB'), ('the', 'DT'), ('board', 'NN'), ('as', 'IN'), ('a', 'DT'), ('nonexecutive', 'JJ'), ('director', 'NN'), ('Nov.', 'NNP'), ('29', 'CD'), ('.', '.'), ('Mr.', 'NNP'), ('Vinken', 'NNP'), ('is', 'VBZ'), ('chairman', 'NN'), ('of', 'IN'), ('Elsevier', 'NNP'), ('N.V.', 'NNP')\n"
       ]
      }
     ],
     "prompt_number": 160
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "sent = treebank.tagged_sents()[0]\n",
      "sent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 161,
       "text": [
        "[('Pierre', 'NNP'),\n",
        " ('Vinken', 'NNP'),\n",
        " (',', ','),\n",
        " ('61', 'CD'),\n",
        " ('years', 'NNS'),\n",
        " ('old', 'JJ'),\n",
        " (',', ','),\n",
        " ('will', 'MD'),\n",
        " ('join', 'VB'),\n",
        " ('the', 'DT'),\n",
        " ('board', 'NN'),\n",
        " ('as', 'IN'),\n",
        " ('a', 'DT'),\n",
        " ('nonexecutive', 'JJ'),\n",
        " ('director', 'NN'),\n",
        " ('Nov.', 'NNP'),\n",
        " ('29', 'CD'),\n",
        " ('.', '.')]"
       ]
      }
     ],
     "prompt_number": 161
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll use the Penn Treebank again to tag our own text."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wordnet\n",
      "-------\n",
      "\n",
      "Wordnet is a different animal.  This corpus contains what are called 'synsets' or synonym rings.  To get the synset for a particular word:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": true,
     "input": [
      "from nltk.corpus import wordnet\n",
      "good_synsets = wordnet.synsets('good')\n",
      "for synset in good_synsets[:10]:\n",
      "    print synset.name\n",
      "    print synset.pos\n",
      "    print synset.definition\n",
      "    print '-' * 40"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "good.n.01\n",
        "n\n",
        "benefit\n",
        "----------------------------------------\n",
        "good.n.02\n",
        "n\n",
        "moral excellence or admirableness\n",
        "----------------------------------------\n",
        "good.n.03\n",
        "n\n",
        "that which is pleasing or valuable or useful\n",
        "----------------------------------------\n",
        "commodity.n.01\n",
        "n\n",
        "articles of commerce\n",
        "----------------------------------------\n",
        "good.a.01\n",
        "a\n",
        "having desirable or positive qualities especially those suitable for a thing specified\n",
        "----------------------------------------\n",
        "full.s.06\n",
        "s\n",
        "having the normally expected amount\n",
        "----------------------------------------\n",
        "good.a.03\n",
        "a\n",
        "morally admirable\n",
        "----------------------------------------\n",
        "estimable.s.02\n",
        "s\n",
        "deserving of esteem and respect\n",
        "----------------------------------------\n",
        "beneficial.s.01\n",
        "s\n",
        "promoting or enhancing well-being\n",
        "----------------------------------------\n",
        "good.s.06\n",
        "s\n",
        "agreeable or pleasing\n",
        "----------------------------------------\n"
       ]
      }
     ],
     "prompt_number": 162
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see that each synset has a definition and a part of speech or POS tag.\n",
      "\n",
      "####Wordnet Part Of Speech (POS) Tags:\n",
      "\n",
      "* n - noun\n",
      "* v - verb\n",
      "* a - adjective\n",
      "* s - adjective satellite\n",
      "* r - adverb\n",
      "\n",
      "Suppose we are only interested in nouns."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "good_nouns = wordnet.synsets('good', pos='n')\n",
      "for noun in good_nouns:\n",
      "    print noun.name\n",
      "    print noun.pos\n",
      "    print noun.definition\n",
      "    print '-' * 40"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "good.n.01\n",
        "n\n",
        "benefit\n",
        "----------------------------------------\n",
        "good.n.02\n",
        "n\n",
        "moral excellence or admirableness\n",
        "----------------------------------------\n",
        "good.n.03\n",
        "n\n",
        "that which is pleasing or valuable or useful\n",
        "----------------------------------------\n",
        "commodity.n.01\n",
        "n\n",
        "articles of commerce\n",
        "----------------------------------------\n"
       ]
      }
     ],
     "prompt_number": 163
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each synset also (potenially) has examples of usage."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for noun in good_nouns:\n",
      "    print noun.definition\n",
      "    if len(noun.examples) == 0:\n",
      "        print '**NO EXAMPLES'\n",
      "    else:\n",
      "        for example in noun.examples:\n",
      "            print example\n",
      "    print '-' * 40"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "benefit\n",
        "for your own good\n",
        "what's the good of worrying?\n",
        "----------------------------------------\n",
        "moral excellence or admirableness\n",
        "there is much good to be found in people\n",
        "----------------------------------------\n",
        "that which is pleasing or valuable or useful\n",
        "weigh the good against the bad\n",
        "among the highest goods of all are happiness and self-realization\n",
        "----------------------------------------\n",
        "articles of commerce\n",
        "**NO EXAMPLES\n",
        "----------------------------------------\n"
       ]
      }
     ],
     "prompt_number": 164
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Each synset can also have a set of lemmas which each can have a set of antonyms.  An antonym is a lemma which has a structure like that of synsets."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "noun = good_nouns[1]\n",
      "\n",
      "for lemma in good_nouns[1].lemmas:\n",
      "    print lemma.name\n",
      "    antonyms = lemma.antonyms()\n",
      "    for antonym in antonyms: print '-', antonym.name"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "good\n",
        "- evil\n",
        "goodness\n",
        "- evilness\n"
       ]
      }
     ],
     "prompt_number": 167
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Since you'll probably have your own domain specific documents that you want to work with, NLTK lets you create custom corpora.  We'll see how to do that later on."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "STRING PROCESSING\n",
      "=================\n",
      "\n",
      "Tokenizing\n",
      "----------\n",
      "\n",
      "Tokenization is the process of breaking a body of text into meaningful pieces.  This is similar to parsing a math equation into tokens such as operators and operands.  With natural language we might want sentences or words. NLTK provides many ways to tokenize text. [Gutenberg](http://gutenberg.org/catalog)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "url = 'http://www.gutenberg.org/cache/epub/1661/pg1661.txt' # The Adventures of Sherlock Holmes\n",
      "import urllib\n",
      "book = urllib.urlopen(url).read()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 168
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Obviously, we could try a naive method."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokens = book.split(' ')\n",
      "print tokens[:100]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['\\xef\\xbb\\xbfProject', \"Gutenberg's\", 'The', 'Adventures', 'of', 'Sherlock', 'Holmes,', 'by', 'Arthur', 'Conan', 'Doyle\\r\\n\\r\\nThis', 'eBook', 'is', 'for', 'the', 'use', 'of', 'anyone', 'anywhere', 'at', 'no', 'cost', 'and', 'with\\r\\nalmost', 'no', 'restrictions', 'whatsoever.', '', 'You', 'may', 'copy', 'it,', 'give', 'it', 'away', 'or\\r\\nre-use', 'it', 'under', 'the', 'terms', 'of', 'the', 'Project', 'Gutenberg', 'License', 'included\\r\\nwith', 'this', 'eBook', 'or', 'online', 'at', 'www.gutenberg.net\\r\\n\\r\\n\\r\\nTitle:', 'The', 'Adventures', 'of', 'Sherlock', 'Holmes\\r\\n\\r\\nAuthor:', 'Arthur', 'Conan', 'Doyle\\r\\n\\r\\nPosting', 'Date:', 'April', '18,', '2011', '[EBook', '#1661]\\r\\nFirst', 'Posted:', 'November', '29,', '2002\\r\\n\\r\\nLanguage:', 'English\\r\\n\\r\\n\\r\\n***', 'START', 'OF', 'THIS', 'PROJECT', 'GUTENBERG', 'EBOOK', 'THE', 'ADVENTURES', 'OF', 'SHERLOCK', 'HOLMES', '***\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nProduced', 'by', 'an', 'anonymous', 'Project', 'Gutenberg', 'volunteer', 'and', 'Jose', 'Menendez\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\n\\r\\nTHE', 'ADVENTURES', 'OF', 'SHERLOCK', 'HOLMES\\r\\n\\r\\nby\\r\\n\\r\\nSIR', 'ARTHUR', 'CONAN', 'DOYLE\\r\\n\\r\\n\\r\\n\\r\\n', '']\n"
       ]
      }
     ],
     "prompt_number": 169
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This has some drawbacks.  The NLTK methods elimate a lot of the noise for us.\n",
      "\n",
      "We can tokenize by sentences."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import sent_tokenize\n",
      "sents = sent_tokenize(book)\n",
      "print 'There are', len(sents), 'sentences'\n",
      "print sents[1002:1004]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 6949 sentences\n",
        "['\"It is quite a three pipe problem, and I\\r\\nbeg that you won\\'t speak to me for fifty minutes.', '\" He curled\\r\\nhimself up in his chair, with his thin knees drawn up to his\\r\\nhawk-like nose, and there he sat with his eyes closed and his\\r\\nblack clay pipe thrusting out like the bill of some strange bird.']\n"
       ]
      }
     ],
     "prompt_number": 170
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And words."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import word_tokenize\n",
      "words = word_tokenize(book)\n",
      "print 'There are', len(words), 'words'\n",
      "print word_tokenize(''.join(sents[1002:1004]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "There are 123100 words\n",
        "['``', 'It', 'is', 'quite', 'a', 'three', 'pipe', 'problem', ',', 'and', 'I', 'beg', 'that', 'you', 'wo', \"n't\", 'speak', 'to', 'me', 'for', 'fifty', 'minutes.', \"''\", 'He', 'curled', 'himself', 'up', 'in', 'his', 'chair', ',', 'with', 'his', 'thin', 'knees', 'drawn', 'up', 'to', 'his', 'hawk-like', 'nose', ',', 'and', 'there', 'he', 'sat', 'with', 'his', 'eyes', 'closed', 'and', 'his', 'black', 'clay', 'pipe', 'thrusting', 'out', 'like', 'the', 'bill', 'of', 'some', 'strange', 'bird', '.']\n"
       ]
      }
     ],
     "prompt_number": 171
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The **`word_tokenize`** function uses a **`TreeBankTokenizer`** to generate tokens.  As you can see it separates contractions.  To avoid this we could use a **`WhitespaceTokenizer`**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import WhitespaceTokenizer\n",
      "tokenizer = WhitespaceTokenizer()\n",
      "print tokenizer.tokenize(''.join(sents[1002:1004]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['\"It', 'is', 'quite', 'a', 'three', 'pipe', 'problem,', 'and', 'I', 'beg', 'that', 'you', \"won't\", 'speak', 'to', 'me', 'for', 'fifty', 'minutes.\"', 'He', 'curled', 'himself', 'up', 'in', 'his', 'chair,', 'with', 'his', 'thin', 'knees', 'drawn', 'up', 'to', 'his', 'hawk-like', 'nose,', 'and', 'there', 'he', 'sat', 'with', 'his', 'eyes', 'closed', 'and', 'his', 'black', 'clay', 'pipe', 'thrusting', 'out', 'like', 'the', 'bill', 'of', 'some', 'strange', 'bird.']\n"
       ]
      }
     ],
     "prompt_number": 172
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The **`WhitespaceTokenizer`** only looks at whitespace characters and does not tokenize punctuation.  The **`WordPunctTokenizer`** will create a token."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.tokenize import WordPunctTokenizer\n",
      "tokenizer = WordPunctTokenizer()\n",
      "print tokenizer.tokenize(''.join(sents[1002:1004]))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['\"', 'It', 'is', 'quite', 'a', 'three', 'pipe', 'problem', ',', 'and', 'I', 'beg', 'that', 'you', 'won', \"'\", 't', 'speak', 'to', 'me', 'for', 'fifty', 'minutes', '.\"', 'He', 'curled', 'himself', 'up', 'in', 'his', 'chair', ',', 'with', 'his', 'thin', 'knees', 'drawn', 'up', 'to', 'his', 'hawk', '-', 'like', 'nose', ',', 'and', 'there', 'he', 'sat', 'with', 'his', 'eyes', 'closed', 'and', 'his', 'black', 'clay', 'pipe', 'thrusting', 'out', 'like', 'the', 'bill', 'of', 'some', 'strange', 'bird', '.']\n"
       ]
      }
     ],
     "prompt_number": 173
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Stemming\n",
      "--------\n",
      "\n",
      "Stemming is the process of reducing inflected words to their root or stem.  Inflection is the modification of a word to express tense, gender, number and others.  For example, the stem of 'running' is 'run'.  The following is a naive approach."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def stem(word):\n",
      "    for suffix in ['ing', 'ly', 'ed', 'ious', 'ies', 'ive', 'es', 's', 'ment']:\n",
      "        if word.endswith(suffix):\n",
      "            return word[:-len(suffix)]\n",
      "    return word\n",
      "\n",
      "print 'runs:', stem('runs')\n",
      "print 'played:', stem('played')\n",
      "print 'playing:', stem('playing')\n",
      "print 'running:', stem('running')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "runs: run\n",
        "played: play\n",
        "playing: play\n",
        "running: runn\n"
       ]
      }
     ],
     "prompt_number": 174
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This does not work for all situations, especially those where the stem is modified.  \n",
      "\n",
      "NLTK gives us several stemmer classes to accomplish this.  The most common is the **`PorterStemmer`**. [Python](http://tartarus.org/martin/PorterStemmer/python.txt)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem import PorterStemmer\n",
      "stemmer = PorterStemmer()\n",
      "print 'talking:', stemmer.stem('talking')\n",
      "print 'talks:', stemmer.stem('talks')\n",
      "print 'talked:', stemmer.stem('talked')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "talking: talk\n",
        "talks: talk\n",
        "talked: talk\n"
       ]
      }
     ],
     "prompt_number": 175
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "NLTK also provides a **`RegexpStemmer`** which is more of a brute force method."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem import RegexpStemmer\n",
      "stemmer = RegexpStemmer('ing')\n",
      "print 'talking:', stemmer.stem('talking')\n",
      "print 'talks:', stemmer.stem('talks')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "talking: talk\n",
        "talks: talks\n"
       ]
      }
     ],
     "prompt_number": 176
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This has the limitation of working only with the specified suffix."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemmer = RegexpStemmer('ed')\n",
      "print 'talked:', stemmer.stem('talked')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "talked: talk\n"
       ]
      }
     ],
     "prompt_number": 177
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "And in instances where the stem itself is modified before adding a suffix:"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemmer = RegexpStemmer('ing')\n",
      "print 'running:', stemmer.stem('running')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "running: runn\n"
       ]
      }
     ],
     "prompt_number": 178
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The **`SnowballStemmer`** is another place that NLTK supports multiple languages. [Conjugation](http://www.spanishdict.com/conjugation)"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.stem import SnowballStemmer\n",
      "stemmer = SnowballStemmer('spanish')\n",
      "print 'hablar:', stemmer.stem('hablar') # to speak\n",
      "print 'hablo:', stemmer.stem('hablo') # I speak"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "hablar: habl\n",
        "hablo: habl\n"
       ]
      }
     ],
     "prompt_number": 179
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "COLLOCATION\n",
      "===========\n",
      "\n",
      "Words that appear in proximity are collocated.  A collocation of two words is a bigram.  NLTK makes this easy."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
      "finder = BigramCollocationFinder.from_words(gutenberg.words('shakespeare-macbeth.txt'))\n",
      "bigram = BigramAssocMeasures()\n",
      "finder.nbest(bigram.pmi, 25) # pmi => pointwise mutual information, scoring method"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 180,
       "text": [
        "[('Accounted', 'dangerous'),\n",
        " ('Acheron', 'Meete'),\n",
        " ('Adde', 'thereto'),\n",
        " ('Adders', 'Forke'),\n",
        " ('Alarme', 'Excite'),\n",
        " ('Antidote', 'Cleanse'),\n",
        " ('Assassination', 'Could'),\n",
        " ('Being', 'vnprepar'),\n",
        " ('Blaspheming', 'Iew'),\n",
        " ('Bonelesse', 'Gummes'),\n",
        " ('Boundlesse', 'intemperance'),\n",
        " ('Charnell', 'houses'),\n",
        " ('Chiefe', 'nourisher'),\n",
        " ('Childrens', 'Ghosts'),\n",
        " ('Clamor', 'rore'),\n",
        " ('Colmes', 'ynch'),\n",
        " ('Could', 'trammell'),\n",
        " ('Craues', 'composition'),\n",
        " ('Damned', 'Fact'),\n",
        " ('Deadmans', 'knell'),\n",
        " ('Deare', 'Duff'),\n",
        " ('Deaths', 'counterfeit'),\n",
        " ('Direnesse', 'familiar'),\n",
        " ('Discomfort', 'swells'),\n",
        " ('Faulcon', 'towring')]"
       ]
      }
     ],
     "prompt_number": 180
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.collocations import TrigramAssocMeasures, TrigramCollocationFinder\n",
      "tri_finder = TrigramCollocationFinder.from_words(gutenberg.words('shakespeare-macbeth.txt'))\n",
      "trigram = TrigramAssocMeasures()\n",
      "tri_finder.nbest(trigram.pmi, 25)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 181,
       "text": [
        "[('Assassination', 'Could', 'trammell'),\n",
        " ('Lifes', 'fitfull', 'Feuer'),\n",
        " ('Mothers', 'womb', 'Vntimely'),\n",
        " ('Obliuious', 'Antidote', 'Cleanse'),\n",
        " ('Saint', 'Colmes', 'ynch'),\n",
        " ('THE', 'TRAGEDIE', 'OF'),\n",
        " ('TRAGEDIE', 'OF', 'MACBETH'),\n",
        " ('William', 'Shakespeare', '1603'),\n",
        " ('Witchcraft', 'celebrates', 'Pale'),\n",
        " ('choppie', 'finger', 'laying'),\n",
        " ('forge', 'Quarrels', 'vniust'),\n",
        " ('grim', 'Alarme', 'Excite'),\n",
        " ('lated', 'Traueller', 'apace'),\n",
        " ('minutely', 'Reuolts', 'vpbraid'),\n",
        " ('multitudinous', 'Seas', 'incarnardine'),\n",
        " ('sad', 'bosomes', 'empty'),\n",
        " ('womb', 'Vntimely', 'ript'),\n",
        " ('yesty', 'Waues', 'Confound'),\n",
        " ('Accounted', 'dangerous', 'folly'),\n",
        " ('After', 'Lifes', 'fitfull'),\n",
        " ('Auarice', 'stickes', 'deeper'),\n",
        " ('Interdiction', 'stands', 'accust'),\n",
        " ('Iourney', 'Soundly', 'inuite'),\n",
        " ('Neptunes', 'Ocean', 'wash'),\n",
        " ('Pale', 'Heccats', 'Offrings')]"
       ]
      }
     ],
     "prompt_number": 181
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "finder.apply_freq_filter(5)\n",
      "finder.nbest(bigram.pmi, 25)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 182,
       "text": [
        "[('three', 'Witches'),\n",
        " ('Scena', 'Secunda'),\n",
        " ('worthy', 'Thane'),\n",
        " ('at', 'once'),\n",
        " ('mine', 'owne'),\n",
        " ('thou', 'art'),\n",
        " ('your', 'Highnesse'),\n",
        " ('no', 'more'),\n",
        " ('Giue', 'me'),\n",
        " ('so', 'much'),\n",
        " ('good', 'Lord'),\n",
        " ('Enter', 'Malcolme'),\n",
        " ('pray', 'you'),\n",
        " ('He', 'hath'),\n",
        " ('We', 'will'),\n",
        " ('my', 'Sword'),\n",
        " ('my', 'Lord'),\n",
        " ('Enter', 'Macbeth'),\n",
        " ('We', 'are'),\n",
        " ('st', 'thou'),\n",
        " ('I', 'am'),\n",
        " ('dare', 'not'),\n",
        " ('and', 'Attendants'),\n",
        " ('could', 'not'),\n",
        " ('make', 'vs')]"
       ]
      }
     ],
     "prompt_number": 182
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def remove_my(word):\n",
      "    return word == 'my'\n",
      "\n",
      "finder.apply_word_filter(remove_my)\n",
      "finder.nbest(bigram.pmi, 25)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 183,
       "text": [
        "[('three', 'Witches'),\n",
        " ('Scena', 'Secunda'),\n",
        " ('worthy', 'Thane'),\n",
        " ('at', 'once'),\n",
        " ('mine', 'owne'),\n",
        " ('thou', 'art'),\n",
        " ('your', 'Highnesse'),\n",
        " ('no', 'more'),\n",
        " ('Giue', 'me'),\n",
        " ('so', 'much'),\n",
        " ('good', 'Lord'),\n",
        " ('Enter', 'Malcolme'),\n",
        " ('pray', 'you'),\n",
        " ('He', 'hath'),\n",
        " ('We', 'will'),\n",
        " ('Enter', 'Macbeth'),\n",
        " ('We', 'are'),\n",
        " ('st', 'thou'),\n",
        " ('I', 'am'),\n",
        " ('dare', 'not'),\n",
        " ('and', 'Attendants'),\n",
        " ('could', 'not'),\n",
        " ('make', 'vs'),\n",
        " ('It', 'is'),\n",
        " ('Thane', 'of')]"
       ]
      }
     ],
     "prompt_number": 183
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "POS TAGGING\n",
      "==========="
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "CLASSIFICATION\n",
      "=============="
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "DOCUMENT SIMILARITY\n",
      "===================\n",
      "\n",
      "For this we can use a formula known as TFIDF or 'term frequency inverse document frequency'.  This tells us the importance of a word in a document in a corpus.\n",
      "\n",
      "The term frequency is the number of times a word appears in a document and thus how important that word is to all others.\n",
      "\n",
      "The inverse document frequency is how important a word is in the corpus.\n",
      "\n",
      "The product of the two is TFIDF."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import math\n",
      "\n",
      "documents = [\n",
      "    'I like to play golf and tennis', \n",
      "    'The local court is a place I like', \n",
      "    'I do not like to play tennis but I like to play golf', \n",
      "    'My neighbor went bowling yesterday'\n",
      "]\n",
      "\n",
      "tokenizer = WhitespaceTokenizer()\n",
      "\n",
      "def tf(term, doc):\n",
      "    words = tokenizer.tokenize(doc)\n",
      "    terms = sum([1 for t in words if t == term])\n",
      "    return float(terms) / float(len(words))\n",
      "\n",
      "def idf(term):\n",
      "    docs = sum([1 for doc in documents if term in tokenizer.tokenize(doc)])\n",
      "    return math.log(float(len(documents)) / float(1 + docs))\n",
      "\n",
      "def tfidf(term, doc):\n",
      "    return tf(term, doc) * idf(term)\n",
      "\n",
      "for doc in documents: \n",
      "    print tfidf('golf', doc) # golf, play, tennis, bowling"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0410974389217\n",
        "0.0\n",
        "0.0221293901886\n",
        "0.0\n"
       ]
      }
     ],
     "prompt_number": 149
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "In a production situation we would have a much larger corpus, filter out stopwords, normalize the text, etc.  But this example fits on a screen.\n",
      "\n",
      "To compute document similarity, scikit-learn provides a nice implementation in **`TfidfVectorizer`**."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "vectorizer = TfidfVectorizer(min_df=1)\n",
      "doc_matrix = vectorizer.fit_transform(documents)\n",
      "(doc_matrix * doc_matrix.T).A"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 145,
       "text": [
        "array([[ 1.        ,  0.08878746,  0.6992982 ,  0.        ],\n",
        "       [ 0.08878746,  1.        ,  0.1063999 ,  0.        ],\n",
        "       [ 0.6992982 ,  0.1063999 ,  1.        ,  0.        ],\n",
        "       [ 0.        ,  0.        ,  0.        ,  1.        ]])"
       ]
      }
     ],
     "prompt_number": 145
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "CLEANING UP YOUR CORPUS\n",
      "======================="
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}